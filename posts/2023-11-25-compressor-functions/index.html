<!doctype html><html lang=en><head><meta charset=utf-8><title>Machine learning with gzip | Tidbits</title><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=description content="I am not sure in which context I originally stumbled across the concept presented in this paper. In that paper, the authors presented a technique in which they would use a lossless compression function (gzip) + compressed distance metric (Normalized Compression Distance) + k-nearest neighbors (k-nn) for text topic classification. I liked this paper when I first learned of it because it is a parameter free model (hold the k hyperparameter), which is against the norm for other popular models in the space."><meta name=author content="Jack Champagne"><meta name=generator content="Hugo 0.71.1"><link href=/index.xml rel=alternate type=application/rss+xml title="Tidbits Feed"><link rel=stylesheet href=/style.7c6d960b20273ed88bc63a60591a57e7739fb21f243b77f27c3bc730ef6205c0.css><script defer src=/script.9df38729991a6ec08d83b60514e841529cebd5ccc800cf8e1f98fb63e73aaa73.js></script></head><body><div class=pure-g><div class="pure-u-1-24 pure-u-md-5-24"></div><div class="pure-u-22-24 pure-u-md-14-24"><div class=navigation><div class="navigation-header clearfix"><div class="pure-menu pure-menu-horizontal"><a class="pure-menu-heading pure-menu-link" href=/>Tidbits</a><ul class="pure-menu-list navigation-header-subtitle pull-end"><li class="pure-menu-item pure-menu-disabled">A Journal</li></ul></div></div><div class=navigation-content><div class="pure-menu pure-menu-horizontal"><ul class=pure-menu-list><li class=pure-menu-item title="All posts"><a class=pure-menu-link href=/posts/>Posts</a></li><li class=pure-menu-item title="All categories"><a class=pure-menu-link href=/categories/>Categories</a></li><li class=pure-menu-item title="All series"><a class=pure-menu-link href=/series/>Series</a></li><li class=pure-menu-item title="All tags"><a class=pure-menu-link href=/tags/>Tags</a></li></ul></div></div></div><div><div><h2 class=post-title>Machine learning with gzip</h2><div class=post-meta><span>Date</span> [
<time datetime=2023-11-25T00:00:00Z>Sat, 25 Nov 2023 00:00:00 UTC</time>
]
<span>Categories</span> [
<a href=/categories/personal>Personal</a>
<a href=/categories/journal>Journal</a>
]
<span>Tags</span> [
<a href=/tags/personal>Personal</a>
<a href=/tags/journal>Journal</a>
]</div></div><div><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><p>I am not sure in which context I originally stumbled across the concept presented in <a href=https://aclanthology.org/2023.findings-acl.426.pdf>this paper</a>. In that paper, the authors presented a technique in which they would use a lossless compression function (gzip) + compressed distance metric (Normalized Compression Distance) + k-nearest neighbors (k-nn) for text topic classification. I liked this paper when I first learned of it because it is a parameter free model (hold the k hyperparameter), which is against the norm for other popular models in the space. I am no expert on NLP (although I have worked with some other areas of machine learning) but something that I can certainly appreciate in the era of multi-billion parameter language transformers is a simple idea applying existing tool in an effective manner.</p><p>One thing of note for this technique however is the runtime complexity of k-nn.
Computing gzip is performing the <a href=https://en.wikipedia.org/wiki/Deflate>DEFLATE algorithm</a> which is a two step process of Huffman coding and then LZ77. A number of places on the internet said that the runtime complexity of this was \( O(n) \), where n is the size of the uncompressed data. I could not find any credible sources doing out the analysis and when I started digging I gfound very few answers (some more information can be found at these two wikipedia articles: <a href=https://en.wikipedia.org/wiki/Huffman_coding#Compression>Huffman coding</a> and <a href=https://en.wikipedia.org/wiki/LZ77_and_LZ78>LZ77</a>).</p><p>Instead I opted for a more empirical approach by just measuring gzip&rsquo;s performance on large bodies of data. Firstly, I generated large files of random data using the following command:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>for arg in 1K 5K 10K 50K 100K 250K 500K 1M 5M 10M 25M 50M 250M 500M 1G; do     head -c $arg &lt;/dev/urandom &gt;&#34;$arg.rand&#34;; done
</code></pre></div><p>and the getting gzip timing by running:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>for arg in 1K 5K 10K 50K 100K 250K 500K 1M 5M 10M 25M 50M 250M 500M 1G; do     time gzip &#34;$arg.rand&#34;; done
</code></pre></div><p>This gave me some data that I have saved here and ran a regression against. It looks like a linear regression is sufficient here and anything lower order (I tried \(log\, n \) for example did not work great). So for now gzip has empirically a linear runtime complexity (tested up to 1 gigabyte). The x-axis represents filesize before compression, the y-axis is seconds to compress (user + sys from time command) and the x-axis has a logarithmic scale.</p><iframe src=https://www.desmos.com/calculator/axfxot4dyf?embed width=500 height=500 style="border:1px solid #ccc" frameborder=0></iframe><p>The other components in this algorithm are interesting too. For example, the authors propose Normalized Compression Distance (NCD) as a means to compute the <code>distance</code> as used by k-nn. This metric is not complicated to compute, the formula for which is given in the paper, where \( C(x) \) is the compressed length of \(x\) and \(xy\) denotes the concatenation of \(x\) and \(y\).</p><p>\[ NCD(x, y) = \frac{C(xy) - \min \left\{C(x), C(y) \right\}}{\max \left\{C(x), C(y)\right\}} \]</p><p>Computing the Normalized Compression Distance between two texts \(x\) and \(y\) will require computing the compressed length of \(xy\) as well.</p><p>And of course, the aspects of an implementation of k-nn with its own runtime complexity as well. This I am choosing not to derive here out of respect for my time and the brevity of this article.</p><p>A few notes here at the end on this paper. They used the metric Normalized Compression Distance as a stand-in for <a href=https://arxiv.org/pdf/1006.3520.pdf>information distance</a> (or \(E(\cdot)\) which is uncomputable because of its dependence on <a href=https://en.wikipedia.org/wiki/Kolmogorov_complexity>Kolmogorov complexity</a>. The idea is that as the compression ratio of gzip becomes higher, it will eventually approach \(K(\cdot)\), thus \(NCD\) approaches \(E\).</p><p>The next note I had here is a video on optimality and related to kolmogorov complexity (specifically on the algorithm proposed in the <a href=https://en.wikipedia.org/wiki/Kolmogorov_complexity#Uncomputability_of_Kolmogorov_complexity>uncomputability section</a> of that wikipedia article: <a href="https://www.youtube.com/watch?v=9ONm1od1QZo">&ldquo;The most powerful (and useless) algorithm&rdquo; - polylog</a> and its percursor: <a href="https://www.youtube.com/watch?v=qrKlPzceeqc">&ldquo;The OPTIMAL algorithm for factoring!&rdquo; - polylog</a>.</p></div></div><div class=footer><div class="pure-menu pure-menu-horizontal"><ul class=pure-menu-list><li class=pure-menu-item title="Theme repository"><a class=pure-menu-link href=https://github.com/jack-champagne/jack-champagne.github.io>GitHub</a></li><li class=pure-menu-item title="Theme page on gohugo.io"><a class=pure-menu-link href=https://themes.gohugo.io/slick>Slick</a></li><li class=pure-menu-item title=Top><a class=pure-menu-link href=#>To Top</a></li><li class=pure-menu-item title="RSS Feed"><a href=/index.xml class=pure-menu-link>RSS</a></li><li class="pure-menu-item fix-cursor-pointer" title="Go to top"><a class=pure-menu-link id=btn-gototop><span class=fix-placement-up>&#8679;&#xfe0e;</span></a></li></ul></div><div class="pure-menu pure-menu-horizontal"><ul class=pure-menu-list><li class="pure-menu-item pure-menu-disabled">&copy; 2024 &mdash; Jack Champagne â€” All rights reserved.</li></ul></div></div></div><div class="pure-u-1-24 pure-u-md-5-24"></div></div></body></html>